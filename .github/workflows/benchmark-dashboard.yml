name: Benchmark Dashboard

on:
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  deployments: write
  pages: write
  id-token: write

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-bench-

      - name: Run benchmarks
        run: |
          # Run benchmarks and output in JSON format
          cargo bench --bench bundling -- --output-format bencher | tee output.txt

          # Extract benchmark results in a format suitable for github-action-benchmark
          # Criterion outputs results that we need to parse
          python3 - <<'EOF' > benchmark-results.json
          import json
          import re

          results = []
          with open('output.txt', 'r') as f:
              content = f.read()

              # Parse Criterion output
              # Example: test bundle_simple_project ... bench:   1,234,567 ns/iter (+/- 123,456)
              pattern = r'test (\S+) \.\.\. bench:\s+([\d,]+) ns/iter'
              matches = re.findall(pattern, content)

              for name, time in matches:
                  # Remove commas from numbers
                  time_ns = int(time.replace(',', ''))
                  results.append({
                      "name": name,
                      "unit": "ns/iter",
                      "value": time_ns
                  })

          # Output in format expected by github-action-benchmark
          output = {
              "results": results
          }

          with open('benchmark-results.json', 'w') as f:
              json.dump(output, f, indent=2)
          EOF

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Serpen Benchmarks
          tool: 'customSmallerIsBetter'
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Alert if performance regresses by more than 10%
          alert-threshold: '110%'
          comment-on-alert: true
          alert-comment-cc-users: '@tinovyatkin'
          # Store results in gh-pages branch
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: benchmarks

      - name: Upload Pages artifact
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-pages-artifact@v3
        with:
          path: benchmarks

  # Deploy to GitHub Pages
  deploy:
    if: github.ref == 'refs/heads/main'
    needs: benchmark
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
