name: Ecosystem Tests

on:
  pull_request:
    paths:
      - 'crates/**'
      - 'ecosystem/**'
      - '.github/workflows/ecosystem-tests.yml'
      - 'Cargo.toml'
      - 'Cargo.lock'
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  pull-requests: write
  issues: write

jobs:
  ecosystem-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Set up uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          activate-environment: true
          python-version: 3.12

      - name: Install Python dependencies
        run: uv sync --all-groups

      - name: Build cribo (release mode)
        run: cargo build --release --bin cribo

      - name: Run ecosystem tests
        run: |
          # Find and run all test_*.py files in ecosystem/scenarios
          test_count=0
          failed_tests=""
          passed_tests=""

          for test_file in ecosystem/scenarios/test_*.py; do
            if [ -f "$test_file" ]; then
              test_name=$(basename "$test_file" .py)
              echo "::group::Running $test_name"

              if python -m ecosystem.scenarios.$test_name; then
                echo "✅ $test_name passed"
                passed_tests="$passed_tests $test_name"
              else
                echo "❌ $test_name failed (continuing with other tests)"
                failed_tests="$failed_tests $test_name"
              fi

              echo "::endgroup::"
              ((test_count++))
            fi
          done

          echo "-----------------------------------"
          echo "Ran $test_count ecosystem test(s)"

          if [ -n "$passed_tests" ]; then
            echo "✅ Passed:$passed_tests"
          fi

          if [ -n "$failed_tests" ]; then
            echo "❌ Failed:$failed_tests"
            echo "::warning::Some ecosystem tests failed:$failed_tests"
            # Don't exit with error - we want to see all results
          fi

          # Save test results for the report
          {
            echo "ECOSYSTEM_TEST_COUNT=$test_count"
            echo "ECOSYSTEM_FAILED_TESTS=$failed_tests"
            echo "ECOSYSTEM_PASSED_TESTS=$passed_tests"
          } >> $GITHUB_ENV

      - name: Run ecosystem benchmarks
        id: bench
        run: |
          # Run benchmarks and save results
          cargo bench --bench ecosystem --features bench -- --save-baseline pr-${{ github.event.pull_request.number || 'main' }}

          # If this is a PR, compare against main
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            # Try to load main baseline for comparison
            if cargo bench --bench ecosystem --features bench -- --load-baseline main &>/dev/null; then
              cargo bench --bench ecosystem --features bench -- --baseline main > benchmark_results.txt 2>&1 || true
            else
              echo "No main baseline found for comparison" > benchmark_results.txt
            fi
          fi

      - name: Generate benchmark report
        if: github.event_name == 'pull_request'
        id: benchmark-report
        run: |
          # Create a markdown report using grouped commands to avoid shellcheck SC2129
          {
            echo "## 📊 Ecosystem Test Results"
            echo ""
            echo "### 📋 Test Status"
            echo ""

            if [ -n "$ECOSYSTEM_PASSED_TESTS" ]; then
              echo "✅ **Passed tests:**"
              for test in $ECOSYSTEM_PASSED_TESTS; do
                echo "  - ${test#test_}"
              done
              echo ""
            fi

            if [ -n "$ECOSYSTEM_FAILED_TESTS" ]; then
              echo "⚠️ **Failed tests:**"
              for test in $ECOSYSTEM_FAILED_TESTS; do
                echo "  - ${test#test_}"
              done
              echo ""
              echo "> Note: These failures are expected while the ecosystem tests are being stabilized."
              echo ""
            fi

            if [ -z "$ECOSYSTEM_PASSED_TESTS" ] && [ -z "$ECOSYSTEM_FAILED_TESTS" ]; then
              echo "No ecosystem tests were run."
              echo ""
            fi

            echo "### 📈 Benchmark Results"
            echo ""

            if [ -f benchmark_results.txt ]; then
              echo '```'
              cat benchmark_results.txt
              echo '```'
            else
              echo "No benchmark comparison available."
            fi

            # Add bundle size information if available
            if [ -d target/tmp ]; then
              echo -e "\n### 📦 Bundle Sizes\n"
              find target/tmp -name "*_bundled.py" -exec ls -lh {} \; | sort | awk '{
                # Extract package name from path like "target/tmp/requests_bundled.py"
                split($9, path_parts, "/")
                filename = path_parts[length(path_parts)]
                gsub(/_bundled\.py$/, "", filename)
                print "- **" filename "**: " $5
              }'
            fi
          } > ecosystem-report.md

      - name: Post PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('ecosystem-report.md', 'utf8');

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('📊 Ecosystem Test Results')
            );

            const body = report + '\n\n<sub>Generated by ecosystem-tests workflow</sub>';

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ecosystem-benchmark-results
          if-no-files-found: ignore
          path: |
            target/criterion/
            ecosystem-report.md
            benchmark_results.txt
